---
title: "Week 5: Bayesian linear regression and introduction to Stan"
date: today
date-format: "DD/MM/YY"
format: pdf
execute: 
  warning: false
  message: false
---

# Introduction

Today we will be starting off using Stan, looking at the kid's test score data set (available in resources for the [Gelman Hill textbook](https://mc-stan.org/rstanarm/reference/rstanarm-datasets.html)). 

```{r}
#install.packages('rstan')
#install.packages('tidyverse')
#install.packages('tidybayes')
#install.packages('here')
#install.packages('skimr')
library(tidyverse)
library(rstan)
library(tidybayes)
library(here)
library(skimr)
```


The data look like this:

```{r}
kidiq <- read_rds(here("kidiq.RDS"))
kidiq
```
As well as the kid's test scores, we have a binary variable indicating whether or not the mother completed high school, the mother's IQ and age. 


# Descriptives

## Question 1

Use plots or tables to show three interesting observations about the data. Remember:

- Explain what your graph/ tables show
- Choose a graph type that's appropriate to the data type
```{r}
skim(kidiq)
```
From dataset, we know that the kid_score, mom_iq, mom_age are numerical variable and the mom_hs is a factor. There are no missing values in the dataset. For kid_score with a mean score of approximately 86.797 and a standard deviation of about 20.41. The 25th percentile is 74, and the median (50th percentile) is 90.For mom_iq, with mean of 100, standard deviation of 15, and percentiles indicating a normal distribution around the mean value. The mean age of mom is around 22.79 years and a standard deviation of approximately 2.70. The median age is 23 and 75th percentile is 25, which indicates the age of mom is young.

```{r}
kidiq |> 
  ggplot(aes(kid_score)) +
  geom_histogram(binwidth = 5, fill = "blue", color = "black") +
  labs(title = "Distribution of Kid Scores", x = "Kid IQ") +
  theme_minimal()
```
The histogram illustrates the distribution of Kid's IQ. The x-axis represents the range of IQ, and the y-axis represents the count of IQ. The histogram shows a unimodal distribution with the highest concentration of IQ around 100. The distribution appears to be symmetric around this peak, suggesting a normal distribution of IQ. The scores extend from near 0 to 150 approximately, but there are very few IQs at these extremes which matches the result of table.
```{r}
kidiq |> 
  ggplot(aes(mom_iq)) +
  geom_histogram(binwidth = 5, fill = "green", color = "black") +
  labs(title = "Distribution of Mom IQ", x = "Mom IQ", y = "Density") +
  theme_minimal()
```

```{r}
kidiq |> 
  ggplot(aes(mom_iq, kid_score)) +
  geom_point() +
  geom_abline()+
  labs(title = "Relationship between Kid Score and Mom IQ", x = "Mom IQ", y = "Kid Score") +
  theme_minimal()

```
The scatter plot shows the relationship between children's IQ on the y-axis and their mothers' IQ on the x-axis. A line has been added to the plot, indicating the trend of the relationship. The regression line indicates a positive correlation, meaning as the Mom's IQ increases, there is a tendency for the kids' IQ to increase as well. However, the spread of the data points around the line suggests some variance, but we still can see the general trend of that.

```{r}
kidiq |>
  ggplot(aes(x = factor(mom_age), y = kid_score)) + 
  geom_boxplot() +
  labs(title = "Relationship between Kid Score and Mom's Age",
       x = "Mom's Age",
       y = "Kid Score") +
  theme_minimal() 

```
The plot is a box plot that illustrates the relationship between children's IQ and their mothers' ages. The distribution of scores across different ages appears relatively consistent, with medians generally around 100 with some variation. For example, the box for age 29 is notably higher and shorter, suggesting that children of 29-year-old mothers have higher median scores and less variability in dataset. Moreover, outliers are present for several ages.
```{r}
kidiq |>
  ggplot(aes(x = factor(mom_hs), y = kid_score)) + 
  geom_boxplot() +
  labs(title = "Relationship between Kid Score and Mom's Education Level",
       x = "Mom's Education Level",
       y = "Kid Score") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 

```

# Estimating mean, no covariates

In class we were trying to estimate the mean and standard deviation of the kid's test scores. The `kids2.stan` file contains a Stan model to do this. If you look at it, you will notice the first `data` chunk lists some inputs that we have to define: the outcome variable `y`, number of observations `N`, and the mean and standard deviation of the prior on `mu`. Let's define all these values in a `data` list.


```{r}
y <- kidiq$kid_score
mu0 <- 80
sigma0 <- 10

# named list to input for stan function
data <- list(y = y, 
             N = length(y), 
             mu0 = mu0,
             sigma0 = sigma0)
```



Now we can run the model:

```{r}
fit <- stan(file = here("kids2.stan"),
            data = data,
            chains = 3,
            iter = 500)
```

Look at the summary

```{r}
fit
```

Traceplot

```{r}
traceplot(fit)
```

All looks fine. 

```{r}
pairs(fit, pars = c("mu", "sigma"))
```

```{r}
stan_dens(fit, separate_chains = TRUE)
```


## Understanding output

What does the model actually give us? A number of samples from the posteriors. To see this, we can use `extract` to get the samples. 

```{r}
post_samples <- extract(fit)
head(post_samples[["mu"]])
```


This is a list, and in this case, each element of the list has 4000 samples. E.g. quickly plot a histogram of mu

```{r}
hist(post_samples[["mu"]])
median(post_samples[["mu"]])
# 95% bayesian credible interval
quantile(post_samples[["mu"]], 0.025)
quantile(post_samples[["mu"]], 0.975)
```



## Plot estimates

There are a bunch of packages, built-in functions that let you plot the estimates from the model, and I encourage you to explore these options (particularly in `bayesplot`, which we will most likely be using later on). I like using the `tidybayes` package, which allows us to easily get the posterior samples in a tidy format (e.g. using gather draws to get in long format). Once we have that, it's easy to just pipe and do ggplots as usual. 


Get the posterior samples for mu and sigma in long format:

```{r}
dsamples <- fit  |> 
  gather_draws(mu, sigma) # gather = long format
dsamples

# wide format
fit  |>  spread_draws(mu, sigma)

# quickly calculate the quantiles using 

dsamples |> 
  median_qi(.width = 0.8)
```

Let's plot the density of the posterior samples for mu and add in the prior distribution

```{r}
dsamples |> 
  filter(.variable == "mu") |> 
  ggplot(aes(.value, color = "posterior")) + geom_density(size = 1) + 
  xlim(c(70, 100)) + 
  stat_function(fun = dnorm, 
        args = list(mean = mu0, 
                    sd = sigma0), 
        aes(colour = 'prior'), size = 1) +
  scale_color_manual(name = "", values = c("prior" = "red", "posterior" = "black")) + 
  ggtitle("Prior and posterior for mean test scores") + 
  xlab("score")
  
```

## Question 2

Change the prior to be much more informative (by changing the standard deviation to be 0.1). Rerun the model. Do the estimates change? Plot the prior and posterior densities. 
```{r}
y <- kidiq$kid_score
mu0 <- 80
sigma0 <- 0.1
# named list to input for stan function
data1 <- list(y = y,
            N = length(y),
            mu0 = mu0,
            sigma0 = sigma0)
```

```{r}
fit1 <- stan(file = "kids2.stan",
              data = data1,
              chains = 3,
              iter = 500)
```


```{r}
print(fit1)
```

```{r}
dsamples <- fit1 |>
  gather_draws(mu, sigma) 

dsamples |>
  filter(.variable == "mu") |>
  ggplot(aes(.value, color = "posterior")) + 
  geom_density(size = 1) +
  xlim(c(70, 100)) +
  stat_function(fun = dnorm,
                args = list(mean = mu0,
                sd = sigma0),
                aes(colour = 'prior'), size = 1) +
  scale_color_manual(name = "", values = c("prior" = "red", "posterior" = "black")) +
  ggtitle("Prior and posterior for mean test scores") +
  xlab("score")
```
By changing the standard deviation to be 0.1, the estimates changed. $\hat\mu$ decreases from 86.71446 to 80.06 and $\hat\sigma$ changes from 20.33741 to 21.41.
\newpage
# Adding covariates

Now let's see how kid's test scores are related to mother's education. We want to run the simple linear regression

$$
Score = \alpha + \beta X
$$
where $X = 1$ if the mother finished high school and zero otherwise. 

`kid3.stan` has the stan model to do this. Notice now we have some inputs related to the design matrix $X$ and the number of covariates (in this case, it's just 1).

Let's get the data we need and run the model. 



```{r}
X <- as.matrix(kidiq$mom_hs, ncol = 1) # force this to be a matrix
K <- 1

data <- list(y = y, N = length(y), 
             X =X, K = K)
fit2 <- stan(file = here("kids3.stan"),
            data = data, 
            iter = 1000)
```


## Question 3

a) Confirm that the estimates of the intercept and slope are comparable to results from `lm()` 
b) Do a `pairs` plot to investigate the joint sample distributions of the slope and intercept. Comment briefly on what you see. Is this potentially a problem?

```{r}
print(fit2)
```


```{r}
model <- lm(kid_score ~ mom_hs, data=kidiq)
summary(model)
```
The estimate for the intercept from lm() is 77.548, and for the bayes regression, the intercept is about 78, 2 values are closed. Meanwhile, the estimate for slope from lm() is 11.771, which is very close to slope inthe bayes regression, which is about 11.30. Thus, the estimates of the intercept and slope are comparable to results from lm().

b) Do a `pairs` plot to investigate the joint sample distributions of the slope and intercept. Comment briefly on what you see. Is this potentially a problem?

```{r}
pairs(fit2, pars = c("alpha", "beta[1]"))
```
From the `pairs` plot, we can find there is a negative linear relationship between the intercept (alpha) and the slope (beta1). Multicollinearity can lead to instability in the estimation of the model parameters, resulting in unreliable and highly variable estimates that are sensitive to minor changes in the model or the data.


## Plotting results

It might be nice to plot the posterior samples of the estimates for the non-high-school and high-school mothered kids. Here's some code that does this: notice the `beta[condition]` syntax. Also notice I'm using `spread_draws`, because it's easier to calculate the estimated effects in wide format

```{r}
fit2 |>
  spread_draws(alpha, beta[k], sigma) |> 
     mutate(nhs = alpha, 
          hs = alpha + beta) |> 
  select(nhs, hs) |> 
  pivot_longer(nhs:hs, names_to = "education", values_to = "estimated_score") |> 
  ggplot(aes(y = education, x = estimated_score)) +
  stat_halfeye() + 
  theme_bw() + 
  ggtitle("Posterior estimates of scores by education level of mother")
  
```


## Question 4

Add in mother's IQ as a covariate and rerun the model. Please  mean center the covariate before putting it into the model. Interpret the coefficient on the (centered) mum's IQ. 

```{r}
kidiq$centered_mom_iq <- kidiq$mom_iq - mean(kidiq$mom_iq)
X <- as.matrix(kidiq[, c("mom_hs", "centered_mom_iq")]) 
K <- 2
data3 <- list(y = y,
            N = length(y),
            X = X,
            K = K
)

fit4 <- stan(file = "kids3.stan",
             data = data3,
             iter = 1000)
```
```{r}
print(fit4)
```
The interpretation of coefficient on the (centered) mum's IQ is that keep other covariates unchanged, on average, an unit increases in the mom's IQ, the kid's score is expected to increase by 0.57.



## Question 5 

Confirm the results from Stan agree with `lm()`
```{r}
model1 <- lm(kid_score ~ mom_hs + centered_mom_iq, data=kidiq)
summary(model1)
```
From the lm() result, the coefficient on the (centered) mum's IQ is 0.56391, which is very close to the results from Stan (0.57). Thus, results from Stan agree with `lm()`


## Question 6

Plot the posterior estimates of scores by education of mother for mothers who have an IQ of 110. 
```{r}
ext_fit <- extract(fit4)
alpha_post <- ext_fit$alpha
beta_post <- ext_fit$beta
sigma<-ext_fit$sigma
b1<-beta_post[,1]
b2<-beta_post[,2]
posterior0 <- alpha_post  +  b1 * 0 + b2  * (110 - mean(kidiq$mom_iq))
posterior1 <- alpha_post  + b1 * 1 + b2 * (110 - mean(kidiq$mom_iq))
df <- data.frame(
  Scores = c(posterior0, posterior1),
  Education = rep(c("education = 0", "education = 1"), each = length(posterior0))
)
ggplot(df, aes(x = Scores, fill = Education)) +
  geom_density(alpha = 0.5) +
  labs(title = "Plot of Posterior Estimates of Scores by Education of 
       Mother for Mothers who have an IQ of 110") +
  scale_fill_manual(values = c("orange", "purple"))
```

## Question 7

Generate and plot (as a histogram) samples from the posterior predictive distribution for a new kid with a mother who graduated high school and has an IQ of 95. 


```{r}
posterior3 <- alpha_post  + b1 + b2 * (95 - mean(kidiq$mom_iq)) + sigma

hist(posterior3, main = "Plot of Posterior Predictive Distribution for a New Kid",
     xlab = "Predicted Scores",col = "pink")

```

